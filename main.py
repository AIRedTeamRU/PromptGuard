# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from app.guard import detect_jailbreak
import uvicorn

app = FastAPI(
    title="PromptGuard API",
    description="üõ°Ô∏è –ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–µ—Ä–≤–æ–ª –¥–ª—è –∑–∞—â–∏—Ç—ã LLM –æ—Ç –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤",
    version="0.1.0"
)

class PromptRequest(BaseModel):
    prompt: str

class GuardResponse(BaseModel):
    is_safe: bool
    risk_score: float
    flags: list[str]
    suggested_rewrite: str

@app.post("/v1/guard-prompt", response_model=GuardResponse)
async def guard_prompt(request: PromptRequest):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ jailbreak, –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–ª–∏ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Ç–µ–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–ª–∞–≥ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é.
    """
    result = detect_jailbreak(request.prompt)
    
    return GuardResponse(
        is_safe=not result["flagged"],
        risk_score=result["risk_score"],
        flags=[result["reason"]] if result["flagged"] else [],
        suggested_rewrite=result["suggested_rewrite"]
    )

@app.get("/")
def read_root():
    return {"message": "Welcome to PromptGuard API ‚Äî your LLM firewall üîê"}

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)